{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time, os, io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import ElementClickInterceptedException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver = \"/Users/Daria/Downloads/chromedriver_4\" # path to the chromedriver\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bottoms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###URLs of bottoms/jeans for scraping\n",
    "\n",
    "#The Cheeky Straight Jean:\n",
    "url1 = 'https://www.everlane.com/products/womens-high-rise-straight-jean-regular-midblue2?collection=womens-jeans'\n",
    "\n",
    "#Authentic Stretch High-Rise Skinny\n",
    "url2 = 'https://www.everlane.com/products/womens-auth-strch-hr-skny-jean-white?collection=womens-jeans'\n",
    "\n",
    "#The Straight Leg Crop\n",
    "url3 = 'https://www.everlane.com/products/womens-straight-leg-crop-bone?collection=womens-bestsellersv2'\n",
    "\n",
    "#The Super-Soft Straight Leg Jean\n",
    "url4 = 'https://www.everlane.com/products/womens-summer-jean-distressed-indigo?collection=womens-jeans'\n",
    "\n",
    "#The Kick Crop Jean\n",
    "url5 = 'https://www.everlane.com/products/womens-kick-crop-jean-black?collection=womens-jeans'\n",
    "\n",
    "#The Cheeky Bootcut Jean\n",
    "url6 = 'https://www.everlane.com/products/womens-cheeky-bootcut-classicbluewash?collection=womens-jeans'\n",
    "\n",
    "#The Curvy Authentic Stretch High-Rise Skinny Jean\n",
    "url7 = 'https://www.everlane.com/products/womens-curvy-as-highrise-skinny-jean-midblue?collection=womens-jeans'\n",
    "\n",
    "#The Modern Flare Jean\n",
    "url8 = 'https://www.everlane.com/products/womens-modern-flare-jean-black?collection=womens-jeans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_urls_bottoms = [url1, url2, url3, url4, url5, url6, url7, url8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for scraping information regarding reviews from list of links\n",
    "## this info will be presented as list of pages that will be parsed later\n",
    "\n",
    "def get_pages(list_urls):\n",
    "    lis_pages = []\n",
    "    for link in list_urls:\n",
    "        lis = []\n",
    "        driver = webdriver.Chrome(chromedriver)\n",
    "        driver.get(link)\n",
    "        time.sleep(2)\n",
    "        #to remove pop-up ad to continue \n",
    "        close_button = driver.find_element_by_xpath('//span[contains(@class, \"product-login-modal__close-button\")]')\n",
    "        close_button.click()\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        #name of product\n",
    "        name_product = soup.find('h1', class_=\"product-heading__name\")\n",
    "        lis += name_product\n",
    "        try:\n",
    "            #price\n",
    "            price = soup.find('span', class_=\"product-heading__price-value\")\n",
    "            lis += price\n",
    "        except TypeError:\n",
    "            price = soup.find('span', class_=\"product-heading__cwyp-price\")\n",
    "            lis += price\n",
    "        try:\n",
    "            reviews = soup.find_all('div', class_=\"review review-index__review\")\n",
    "            lis += reviews\n",
    "            #number of pages with review\n",
    "            num = int(soup.find('div', class_='pagination-controls').find_all(class_=\"pagination-controls__page\")[:-1][-1].text)-1\n",
    "            if num > 300:\n",
    "                num = 300\n",
    "            for i in range(num):\n",
    "                try:\n",
    "                    search_button = driver.find_element_by_xpath('//button[contains(@class, \"pagination-controls__page pagination-controls__page--next\")]')\n",
    "                    search_button.click()\n",
    "                    #waiting 3 sec for loading next page review\n",
    "                    time.sleep(3) \n",
    "                    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    reviews = soup.find_all('div', class_=\"review review-index__review\") \n",
    "                    lis += reviews\n",
    "                except ElementClickInterceptedException:\n",
    "                    continue\n",
    "            driver.quit()\n",
    "        except (SyntaxError, AttributeError):\n",
    "            driver.quit()\n",
    "            continue\n",
    "        lis_pages.append(lis)\n",
    "        \n",
    "    return lis_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dictionary from list of pages with reviews\n",
    "\n",
    "def get_dict(list_pages_all):\n",
    "    dict_info = {}\n",
    "    for j,list_pages in enumerate(list_pages_all):\n",
    "        #product name\n",
    "        product_name = list_pages[0].text\n",
    "        #price\n",
    "        price = list_pages[1]\n",
    "        for i, review in enumerate(list_pages[2:]):\n",
    "            #review title\n",
    "            review_title = review.find(class_=\"review__title\").text\n",
    "            #the review body\n",
    "            review_body = review.find(class_=\"review__body\").text\n",
    "            #Date\n",
    "            date_posted = review.find('span', class_=\"review__attribution\").text.split(',',1)[1].strip()\n",
    "            #Name\n",
    "            name = review.find('span', class_=\"review__attribution\").text.split(',',1)[0].strip()\n",
    "            rating = str(review.find(class_=\"review__rating\"))[35:39]\n",
    "\n",
    "            # Some of the information on the Website regarding Size, Height, Weight is missing\n",
    "            ## Creating exceptions (Try - Ecxept) for each of these categories for handling some errors\n",
    "\n",
    "            try:\n",
    "            #Usual Size\n",
    "                usual_size = review.find_all('span', class_=\"review__info-attribute\")[0].text\n",
    "                if \"Usual\" in usual_size:\n",
    "                    pass\n",
    "                else:\n",
    "                    usual_size = np.nan\n",
    "            except IndexError:\n",
    "                usual_size = np.nan\n",
    "            try:\n",
    "                #Size purchased\n",
    "                size_purchased = review.find_all('span', class_=\"review__info-attribute\")[1].text\n",
    "                if \"Purchased\" in size_purchased:\n",
    "                    pass\n",
    "                else:\n",
    "                    size_purchased = np.nan\n",
    "            except IndexError:\n",
    "                size_purchased = np.nan\n",
    "            try:    \n",
    "                #Height\n",
    "                height = review.find_all('span', class_=\"review__info-attribute\")[2].text\n",
    "\n",
    "            except IndexError:\n",
    "                height = np.nan\n",
    "            try:\n",
    "            #Weight\n",
    "                weight = review.find_all('span', class_=\"review__info-attribute\")[3].text\n",
    "            except IndexError:\n",
    "                weight = np.nan\n",
    "            try:\n",
    "                #Bust size, if I were scraping tops and tees\n",
    "                bust = review.find_all('span', class_=\"review__info-attribute\")[4].text\n",
    "            except IndexError:\n",
    "                bust = np.nan\n",
    "            dict_info[j,i] = ([name, review_title, review_body, rating, date_posted, usual_size, size_purchased, height, weight,\n",
    "                             bust, product_name, price])\n",
    "    return dict_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making DataFrame out of dictionary, correcting some info\n",
    "\n",
    "def dict_into_df(list_pages):\n",
    "    df = pd.DataFrame(list_pages).T\n",
    "    df = df.reset_index()\n",
    "    df.drop(['level_0', 'level_1'], axis=1, inplace=True)\n",
    "    df.columns = ['name', 'review_title', 'review_body', 'rating', 'date_posted', 'usual_size', 'size_purchased', 'height', 'weight',\n",
    "                         'bust', 'product_name', 'price']\n",
    "    \n",
    "    #correcting indexes\n",
    "    \n",
    "    ##1. Correcting height by moving bust\n",
    "    indexes = list(df.height[df['weight'].str.contains(\"Height\", na = False)].index)\n",
    "\n",
    "    #moving weight from height column to the weight column\n",
    "    for idx in indexes:\n",
    "        df.loc[idx]['height'] = df.loc[idx]['weight']\n",
    "\n",
    "    #setting 0 for height column where weight was previously set\n",
    "    for idx in indexes:\n",
    "        df.loc[idx]['weight'] = np.nan\n",
    "        \n",
    "    ##2. Correcting weight by moving height\n",
    "    indexes = list(df.weight[df['height'].str.contains(\"Weight\", na = False)].index)\n",
    "\n",
    "    #moving weight from height column to the weight column\n",
    "    for idx in indexes:\n",
    "        df.loc[idx]['weight'] = df.loc[idx]['height']\n",
    "\n",
    "    #setting 0 for height column where weight was previously set\n",
    "    for idx in indexes:\n",
    "        df.loc[idx]['height'] = np.nan\n",
    "    \n",
    "    ##3. Correcting bust by moving weight into right column\n",
    "    indexes = list(df.weight[df['bust'].str.contains(\"Weight\", na = False)].index)\n",
    "\n",
    "    #moving weight from height column to the weight column\n",
    "    for idx in indexes:\n",
    "        df.loc[idx]['weight'] = df.loc[idx]['bust']\n",
    "\n",
    "    #setting 0 for height column where weight was previously set\n",
    "    for idx in indexes:\n",
    "        df.loc[idx]['bust'] = np.nan\n",
    "        \n",
    "    ##4. Correcting weight by moving height\n",
    "    indexes = list(df.bust[df['height'].str.contains(\"Bust\", na = False)].index)\n",
    "\n",
    "    #moving weight from height column to the weight column\n",
    "    for idx in indexes:\n",
    "        df.loc[idx]['bust'] = df.loc[idx]['height']\n",
    "\n",
    "    #setting 0 for height column where weight was previously set\n",
    "    for idx in indexes:\n",
    "        df.loc[idx]['height'] = np.nan\n",
    "    \n",
    "    ##5. Correcting weight by moving height\n",
    "    indexes = list(df.height[df['height'].str.contains(\"Fit\", na = False)].index)\n",
    "\n",
    "\n",
    "    #setting 0 for height column where weight was previously set\n",
    "    for idx in indexes:\n",
    "        df.loc[idx]['height'] = np.nan\n",
    "    \n",
    "    ##6. Correcting weight by moving height\n",
    "    indexes = list(df.bust[df['weight'].str.contains(\"Bust\", na = False)].index)\n",
    "\n",
    "    #moving weight from height column to the weight column\n",
    "    for idx in indexes:\n",
    "        df.loc[idx]['bust'] = df.loc[idx]['weight']\n",
    "\n",
    "    #setting 0 for height column where weight was previously set\n",
    "    for idx in indexes:\n",
    "        df.loc[idx]['weight'] = np.nan\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting list of pages\n",
    "\n",
    "lis_pages_bottoms = get_pages(list_urls_bottoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dictionary\n",
    "\n",
    "bottoms_dict = get_dict(lis_pages_bottoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making DataFrame\n",
    "\n",
    "df_bottoms = dict_into_df(bottoms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle!\n",
    "\n",
    "df_bottoms.to_pickle(\"./df_bottoms.pkl\")\n",
    "\n",
    "##df_bottoms = pd.read_pickle(\"./df_bottoms.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shoes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of url for shoes\n",
    "\n",
    "# The Day Glove\n",
    "url01 = 'https://www.everlane.com/products/womens-day-glove-mocha?collection=womens-shoes'\n",
    "\n",
    "# The Day Heel\n",
    "url02 = 'https://www.everlane.com/products/womens-day-heel-burnt-light-taupe?collection=womens-shoes'\n",
    "\n",
    "# The Editor Slingback\n",
    "url03 = 'https://www.everlane.com/products/womens-editor-slingback-rosewood-suede?collection=womens-sale'\n",
    "\n",
    "# The Trainer\n",
    "url04 = 'https://www.everlane.com/products/womens-trainer-grey?collection=womens-sale'\n",
    "\n",
    "# The Square Toe Chelsea Boot\n",
    "url05 = 'https://www.everlane.com/products/womens-square-toe-chelsea-boot-biscuit?collection=womens-shoes'\n",
    "\n",
    "# The Modern Babo\n",
    "url06 = 'https://www.everlane.com/products/womens-modern-babo-black?collection=womens-shoes'\n",
    "\n",
    "# The Day Loafer\n",
    "url07 = 'https://www.everlane.com/products/womens-day-loafer-merlot?collection=womens-shoes'\n",
    "\n",
    "# The Editor Heel\n",
    "url08 = 'https://www.everlane.com/products/womens-editor-heel-navy-suede?collection=womens-sale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_urls_shoes = [url01, url02, url03, url04, url05, url06, url07, url08]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting list of pages\n",
    "\n",
    "lis_pages_shoes = get_pages(list_urls_shoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dictionary\n",
    "\n",
    "shoes_dict = get_dict(lis_pages_shoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making DataFrame\n",
    "\n",
    "df_shoes = dict_into_df(shoes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle!\n",
    "\n",
    "df_shoes.to_pickle(\"./df_shoes.pkl\")\n",
    "\n",
    "##df_shoes = pd.read_pickle(\"./df_shoes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Disclaimer:_** <br>\n",
    "The scraping process that I was doing during the Project was performed in another jupyter notebook. \n",
    "This notebook represents a cleaner version as if I were scraping once again.<br>\n",
    "(Since scraping is time consuming I decided not to re-scrape all over again, but show code and process)<br>\n",
    "Also, the way of scraping provided in this notebook may vary with time, for example, if the website has changed (regarding pop-up windows and etc.)<br>\n",
    "This code worked as of late May 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
